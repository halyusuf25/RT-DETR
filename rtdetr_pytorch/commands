module load python/3.11/3.11.4
module load cuda12/12.4.1 
source .venv/bin/activate
cd rtdetr_pytorch/

# training on single-gpu
export CUDA_VISIBLE_DEVICES=0
python tools/train.py -c configs/rtdetr/rtdetr_r50vd_6x_coco.yml


# train on multi-gpu
export CUDA_VISIBLE_DEVICES=0,1,2,3
torchrun --nproc_per_node=4 tools/train.py -c configs/rtdetr/rtdetr_r50vd_6x_coco.yml


CUDA_LAUNCH_BLOCKING=1 TORCH_DISTRIBUTED_DEBUG=INFO CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 tools/train.py --nnode=1 -c configs/rtdetr/rtdetr_r50vd_6x_coco.yml
CUDA_VISIBLE_DEVICES=0,1,2,3 python tools/train.py -c configs/rtdetr/rtdetr_r50vd_6x_coco.yml

OMP_NUM_THREADS=128 torchrun --nproc_per_node=4 tools/train.py -c configs/rtdetr/rtdetr_r50vd_6x_coco.yml

# val on multi-gpu
export CUDA_VISIBLE_DEVICES=0,1,2,3
torchrun --nproc_per_node=4 tools/train.py \
            -c configs/rtdetr/rtdetr_r50vd_6x_coco.yml \
            -r /groups/eungjoolee/data/halyusuf/pretrined_model/rtdetr/rtdetr_r50vd_6x_coco_from_paddle.pth \
            --test-only

# val on multi-gpu
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 tools/train.py \
            -c configs/rtdetr/rtdetr_r50vd_6x_coco.yml \
            -r /groups/eungjoolee/data/halyusuf/pretrined_model/rtdetr/rtdetr_r50vd_6x_coco_from_paddle.pth \
            --test-only

CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=4 tools/train.py \
            -c configs/rtdetr/rtdetr_r50vd_6x_coco.yml \
            -r /groups/eungjoolee/data/halyusuf/pretrined_model/rtdetr/rtdetr_r50vd_6x_coco_from_paddle.pth \
            --test-only

CUDA_VISIBLE_DEVICES=0,1,2,3 python tools/train.py \
            -c configs/rtdetr/rtdetr_r50vd_6x_coco.yml \
            -r /groups/eungjoolee/data/halyusuf/pretrined_model/rtdetr/rtdetr_r50vd_6x_coco_from_paddle.pth \
            --test-only



CUDA_LAUNCH_BLOCKING=1 TORCH_DISTRIBUTED_DEBUG=INFO CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 test_script.py 


torchrun --nproc_per_node=4 tools/train.py -c configs/rtdetr/rtdetr_r50vd_6x_coco.yml
OMP_NUM_THREADS=128 torchrun --nproc_per_node=4 tools/train.py -c configs/rtdetr/rtdetr_r50vd_6x_coco.yml
torchrun --nproc_per_node=4 --master_port=32345 tools/train.py -c configs/rtdetr/rtdetr_r50vd_top100.yml